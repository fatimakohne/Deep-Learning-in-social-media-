{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7cf28d1",
   "metadata": {},
   "source": [
    "<h1> User recommendation system based on sentiment analysis and matrix factorization</h1>\n",
    "\n",
    "This notebook represents the code for the user recommendation system. The code includes crawling tweets from twitter, performing a sentiment analysis based on those tweets, using matrix factorization to estimate missing ratings and computing cosine similarity to measure user similarity.\n",
    "    \n",
    "Running the notebook was tested on windows operating system using an anaconda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "870f7096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in c:\\users\\johan\\anaconda3\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\users\\johan\\anaconda3\\lib\\site-packages (from tweepy) (2.25.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\johan\\anaconda3\\lib\\site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\johan\\anaconda3\\lib\\site-packages (from tweepy) (1.15.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\johan\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\johan\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\johan\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\johan\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\johan\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\johan\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
      "Requirement already satisfied: textblob in c:\\users\\johan\\anaconda3\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\johan\\anaconda3\\lib\\site-packages (from textblob) (3.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\johan\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.59.0)\n",
      "Requirement already satisfied: regex in c:\\users\\johan\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.4.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\johan\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\johan\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "# install the necessary libraries tweepy and textblob\n",
    "# the other libraries, such as pandas or numpy, are already pre-installed through anaconda\n",
    "!pip install tweepy\n",
    "!pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a531976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all needed libraries\n",
    "import tweepy as tw\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import numpy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9d86337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys, which are needed to authenticate this application to my twitter developer account\n",
    "consumer_key= \"KZrWgSozl4KlNdO8E78C7MQYV\"\n",
    "consumer_secret= \"ANvxLJ1yxzwaGmHP4psLp66YI7rfi0lwJWCKIbPQ5u7HIpWq34\"\n",
    "access_token= \"2410718785-PLIAN7lUp8Bm6AN4jsmEgY35f9Ha8Ubgv2N3oy9\"\n",
    "access_token_secret= \"p5Yn0UpVTV1eDeulItW9zew71RHkNMwVzfTLqXLxwYofN\"\n",
    "\n",
    "# authenticate tweepy to the twitter developer account\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f279cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize global variables\n",
    "# matrix stores the user-rating matrix, which will be created through sentiment analysis\n",
    "matrix = []\n",
    "# a list of all users, which ratings are stored in the user-rating matrix\n",
    "# the index of a user in this list represents the index of the users ratings in the matrix\n",
    "users = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a5dde9",
   "metadata": {},
   "source": [
    "<h2> Crawling data from twitter  </h2>\n",
    "\n",
    " The following cell deals about crawling tweets from twitter. The search strings are being initialized and used by tweepy to collect tweets, which match the given search string. Only tweets created after 2021-08-15 are being considered. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dce504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashtag search strings for each topic\n",
    "# retweets are being filtered out\n",
    "climatechange = \"#climate+change -filter:retweets\"\n",
    "movies = \"#movies -filter:retweets\"\n",
    "football = \"#football -filter:retweets\"\n",
    "coronavirus = \"#coronavirus -filter:retweets\"\n",
    "cars = \"#cars -filter:retweets\"\n",
    "date_since = \"2021-08-15\"\n",
    "# the number of tweets, which will be collected per topic\n",
    "number_of_tweets = 100\n",
    "\n",
    "# collect tweets per topic using tweepy with the search strings\n",
    "# language of the tweets must be english\n",
    "climateTweets = tw.Cursor(api.search,\n",
    "              q=climatechange,\n",
    "              lang=\"en\",\n",
    "              since=date_since).items(number_of_tweets)\n",
    "\n",
    "movieTweets = tw.Cursor(api.search,\n",
    "              q=movies,\n",
    "              lang=\"en\",\n",
    "              since=date_since).items(number_of_tweets)\n",
    "    \n",
    "footballTweets = tw.Cursor(api.search,\n",
    "              q=football,\n",
    "              lang=\"en\",\n",
    "              since=date_since).items(number_of_tweets)\n",
    "\n",
    "coronaviursTweets = tw.Cursor(api.search,\n",
    "              q=coronavirus,\n",
    "              lang=\"en\",\n",
    "              since=date_since).items(number_of_tweets)\n",
    "\n",
    "carTweets = tw.Cursor(api.search,\n",
    "              q=cars,\n",
    "              lang=\"en\",\n",
    "              since=date_since).items(number_of_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "330cffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method, used before the sentiment analysis\n",
    "# removes all urls in a text string\n",
    "def remove_url(txt):\n",
    "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", txt).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87926beb",
   "metadata": {},
   "source": [
    "<h2>Sentiment Analysis</h2>\n",
    "\n",
    "The following cells deal about performing a sentiment analysis based on the collected tweets. The python library TextBlob is used to compute the sentiment value. The results of the sentiment analysis are written into a user-rating matrix. The matrix stores each user as a row and the corresponding sentiment values for the topics as columns. If a user did not tweet about a topic, the sentiment value is 0 for that topic. One method for each topic is implemented, which are structured the same but the sentiment values are being stored into different columns, depending on the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ac2b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods for performing the sentiment analysis\n",
    "# one method for each topic\n",
    "def analyzeClimatechangeTweets(tweets):\n",
    "    # using the global variables\n",
    "    global matrix\n",
    "    global users\n",
    "    # iterate over all tweets of this topic\n",
    "    for tweet in tweets:\n",
    "        # save the user of the tweet\n",
    "        user = tweet.user.screen_name\n",
    "        # remove all URLs from the tweet\n",
    "        text = remove_url(tweet.text)\n",
    "        # create a TextBlob object, which performs the sentiment analysis\n",
    "        sentiment_object = TextBlob(text)\n",
    "        # read the sentiment polarity value from the TextBlob object\n",
    "        # the value is a float within the range [-1.0, 1.0]\n",
    "        sentiment_value_temp = sentiment_object.sentiment.polarity\n",
    "        # rescale the sentiment value to a range of [1.0, 5.0]\n",
    "        sentiment_value = (((sentiment_value_temp - (-1)) * 4) / 2) + 1\n",
    "\n",
    "        # check whether there exists entries for the same user in the user-rating matrix\n",
    "        # the reason for that is that we want one single row for one user and not one new row per tweet\n",
    "        if user in users:\n",
    "            # get the index of the user in the user-rating matrix\n",
    "            i = users.index(user)\n",
    "            # store the sentiment value in the first column (climate change tweets are being stored in the first column) \n",
    "            # of the row\n",
    "            matrix[i][0] = sentiment_value\n",
    "        # otherwise this is the first entry of the user    \n",
    "        else:\n",
    "            # append a new row for the user in the user rating matrix and save the sentiment value in the first column\n",
    "            # the other columns are zero, because this is the first entry for this user and we do not have any tweets\n",
    "            # regarding the other topics\n",
    "            matrix.append([sentiment_value,0,0,0,0])\n",
    "            # to remember that we now have an entry for that user, save that user in the user list\n",
    "            users.append(user)\n",
    "\n",
    "def analyzeMovieTweets(tweets):\n",
    "    # using the global variables\n",
    "    global matrix\n",
    "    global users\n",
    "    # iterate over all tweets of this topic\n",
    "    for tweet in tweets:\n",
    "        # save the user of the tweet\n",
    "        user = tweet.user.screen_name\n",
    "        # remove all URLs from the tweet\n",
    "        text = remove_url(tweet.text)\n",
    "        # create a TextBlob object, which performs the sentiment analysis\n",
    "        sentiment_object = TextBlob(text)\n",
    "        # read the sentiment polarity value from the TextBlob object\n",
    "        # the value is a float within the range [-1.0, 1.0]\n",
    "        sentiment_value_temp = sentiment_object.sentiment.polarity\n",
    "        # rescale the sentiment value to a range of [1.0, 5.0]\n",
    "        sentiment_value = (((sentiment_value_temp - (-1)) * 4) / 2) + 1\n",
    "\n",
    "        # check whether there exists entries for the same user in the user-rating matrix\n",
    "        # the reason for that is that we want one single row for one user and not one new row per tweet\n",
    "        if user in users:\n",
    "            # get the index of the user in the user-rating matrix\n",
    "            i = users.index(user)\n",
    "            # store the sentiment value in the second column (movie tweets are being stored in the second column) \n",
    "            # of the row\n",
    "            matrix[i][1] = sentiment_value\n",
    "        # otherwise this is the first entry of the user      \n",
    "        else:\n",
    "            # append a new row for the user in the user rating matrix and save the sentiment value in the second column\n",
    "            # the other columns are zero, because this is the first entry for this user and we do not have any tweets\n",
    "            # regarding the other topics\n",
    "            matrix.append([0,sentiment_value,0,0,0])\n",
    "            # to remember that we now have an entry for that user, save that user in the user list\n",
    "            users.append(user)\n",
    "\n",
    "def analyzeFootballTweets(tweets):\n",
    "    # using the global variables\n",
    "    global matrix\n",
    "    global users\n",
    "    # iterate over all tweets of this topic\n",
    "    for tweet in tweets:\n",
    "        # save the user of the tweet\n",
    "        user = tweet.user.screen_name\n",
    "        # remove all URLs from the tweet\n",
    "        text = remove_url(tweet.text)\n",
    "        # create a TextBlob object, which performs the sentiment analysis\n",
    "        sentiment_object = TextBlob(text)\n",
    "        # read the sentiment polarity value from the TextBlob object\n",
    "        # the value is a float within the range [-1.0, 1.0]\n",
    "        sentiment_value_temp = sentiment_object.sentiment.polarity\n",
    "        # rescale the sentiment value to a range of [1.0, 5.0]\n",
    "        sentiment_value = (((sentiment_value_temp - (-1)) * 4) / 2) + 1\n",
    "\n",
    "        # check whether there exists entries for the same user in the user-rating matrix\n",
    "        # the reason for that is that we want one single row for one user and not one new row per tweet\n",
    "        if user in users:\n",
    "            # get the index of the user in the user-rating matrix\n",
    "            i = users.index(user)\n",
    "            # store the sentiment value in the third column (football tweets are being stored in the third column) \n",
    "            # of the row\n",
    "            matrix[i][2] = sentiment_value\n",
    "        # otherwise this is the first entry of the user\n",
    "        else:\n",
    "            # append a new row for the user in the user rating matrix and save the sentiment value in the third column\n",
    "            # the other columns are zero, because this is the first entry for this user and we do not have any tweets\n",
    "            # regarding the other topics\n",
    "            matrix.append([0,0,sentiment_value,0,0])\n",
    "            # to remember that we now have an entry for that user, save that user in the user list\n",
    "            users.append(user)\n",
    "\n",
    "def analyzeCoronavirusTweets(tweets):\n",
    "    # using the global variables\n",
    "    global matrix\n",
    "    global users\n",
    "    # iterate over all tweets of this topic\n",
    "    for tweet in tweets:\n",
    "        # save the user of the tweet\n",
    "        user = tweet.user.screen_name\n",
    "        # remove all URLs from the tweet\n",
    "        text = remove_url(tweet.text)\n",
    "        # create a TextBlob object, which performs the sentiment analysis\n",
    "        sentiment_object = TextBlob(text)\n",
    "        # read the sentiment polarity value from the TextBlob object\n",
    "        # the value is a float within the range [-1.0, 1.0]\n",
    "        sentiment_value_temp = sentiment_object.sentiment.polarity\n",
    "        # rescale the sentiment value to a range of [1.0, 5.0]\n",
    "        sentiment_value = (((sentiment_value_temp - (-1)) * 4) / 2) + 1\n",
    "\n",
    "        # check whether there exists entries for the same user in the user-rating matrix\n",
    "        # the reason for that is that we want one single row for one user and not one new row per tweet\n",
    "        if user in users:\n",
    "            # get the index of the user in the user-rating matrix\n",
    "            i = users.index(user)\n",
    "            # store the sentiment value in the fourth column (coronavirus tweets are being stored in the fourth column) \n",
    "            # of the row\n",
    "            matrix[i][3] = sentiment_value\n",
    "        # otherwise this is the first entry of the user\n",
    "        else:\n",
    "            # append a new row for the user in the user rating matrix and save the sentiment value in the fourth column\n",
    "            # the other columns are zero, because this is the first entry for this user and we do not have any tweets\n",
    "            # regarding the other topics\n",
    "            matrix.append([0,0,0,sentiment_value,0])\n",
    "            # to remember that we now have an entry for that user, save that user in the user list\n",
    "            users.append(user)\n",
    "            \n",
    "def analyzeCarTweets(tweets):\n",
    "    # using the global variables\n",
    "    global matrix\n",
    "    global users\n",
    "    # iterate over all tweets of this topic\n",
    "    for tweet in tweets:\n",
    "        # save the user of the tweet\n",
    "        user = tweet.user.screen_name\n",
    "        # remove all URLs from the tweet\n",
    "        text = remove_url(tweet.text)\n",
    "        # create a TextBlob object, which performs the sentiment analysis\n",
    "        sentiment_object = TextBlob(text)\n",
    "        # read the sentiment polarity value from the TextBlob object\n",
    "        # the value is a float within the range [-1.0, 1.0]\n",
    "        sentiment_value_temp = sentiment_object.sentiment.polarity\n",
    "        # rescale the sentiment value to a range of [1.0, 5.0]\n",
    "        sentiment_value = (((sentiment_value_temp - (-1)) * 4) / 2) + 1\n",
    "\n",
    "        # check whether there exists entries for the same user in the user-rating matrix\n",
    "        # the reason for that is that we want one single row for one user and not one new row per tweet\n",
    "        if user in users:\n",
    "            # get the index of the user in the user-rating matrix\n",
    "            i = users.index(user)\n",
    "            # store the sentiment value in the fifth column (car tweets are being stored in the fifth column) \n",
    "            # of the row\n",
    "            matrix[i][4] = sentiment_value\n",
    "        # otherwise this is the first entry of the user\n",
    "        else:\n",
    "            # append a new row for the user in the user rating matrix and save the sentiment value in the fifth column\n",
    "            # the other columns are zero, because this is the first entry for this user and we do not have any tweets\n",
    "            # regarding the other topics\n",
    "            matrix.append([0,0,0,0,sentiment_value])\n",
    "            # to remember that we now have an entry for that user, save that user in the user list\n",
    "            users.append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51d380c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sentiment analysis for climate change tweets!\n",
      "Sentiment analysis done!\n"
     ]
    }
   ],
   "source": [
    "# perform the sentiment analysis for each climate change tweet\n",
    "# the results are being saved in the user-rating matrix\n",
    "print(\"Starting sentiment analysis for climate change tweets!\")\n",
    "analyzeClimatechangeTweets(climateTweets)\n",
    "print(\"Sentiment analysis done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18c06ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sentiment analysis for movie tweets!\n",
      "Sentiment analysis done!\n"
     ]
    }
   ],
   "source": [
    "# perform the sentiment analysis for each movie tweet\n",
    "# the results are being saved in the user-rating matrix\n",
    "print(\"Starting sentiment analysis for movie tweets!\")\n",
    "analyzeMovieTweets(movieTweets)\n",
    "print(\"Sentiment analysis done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44874e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sentiment analysis for football tweets!\n",
      "Sentiment analysis done!\n"
     ]
    }
   ],
   "source": [
    "# perform the sentiment analysis for each football tweet\n",
    "# the results are being saved in the user-rating matrix\n",
    "print(\"Starting sentiment analysis for football tweets!\")\n",
    "analyzeFootballTweets(footballTweets)\n",
    "print(\"Sentiment analysis done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b91c43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sentiment analysis for coronavirus tweets!\n",
      "Sentiment analysis done!\n"
     ]
    }
   ],
   "source": [
    "# perform the sentiment analysis for each coronavirus tweet\n",
    "# the results are being saved in the user-rating matrix\n",
    "print(\"Starting sentiment analysis for coronavirus tweets!\")\n",
    "analyzeCoronavirusTweets(coronaviursTweets)\n",
    "print(\"Sentiment analysis done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd107378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting sentiment analysis for car tweets!\n",
      "Sentiment analysis done!\n"
     ]
    }
   ],
   "source": [
    "# perform the sentiment analysis for each car tweet\n",
    "# the results are being saved in the user-rating matrix\n",
    "print(\"Starting sentiment analysis for car tweets!\")\n",
    "analyzeCarTweets(carTweets)\n",
    "print(\"Sentiment analysis done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e13ce9a",
   "metadata": {},
   "source": [
    "<h2>Matrix Factorization</h2>\n",
    "\n",
    "The following cells are implementing a matrix factorization, which is used for predicting user ratings on topics. The user-feature matrix and the topic-feature matrix are being initialized randomly. The goal is to discover 10 latent features. Gradient descent, with a maximum of 500 iterations, a learning rate of 0.0002 and a regularization parameter of 0.02, computes the difference of the ratings and tries to find a local minimum of the difference. At the end, the user-rating matrix with the estimated ratings is being constructed by computing the dot product of the user-feature matrix P and the topic-feature matrix Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c648e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the method for performing the matrix factorization\n",
    "# input is the user-rating matrix, which has been created by the sentiment analysis\n",
    "def mft(R):\n",
    "    # N denotes the number of users\n",
    "    N = len(R)  \n",
    "    # M denotes the number of topics\n",
    "    M = len(R[0])\n",
    "    # K denotes the number of latent features\n",
    "    K = 10\n",
    "    \n",
    "    # P is the user feature matrix\n",
    "    # Q is the topic feature matrix\n",
    "    # both matrices are being initialized randomly\n",
    "    P = numpy.random.rand(N,K)\n",
    "    Q = numpy.random.rand(M,K)\n",
    "    \n",
    "    Q = Q.T\n",
    "    # alpha denotes the learning rate used by gradient descent\n",
    "    alpha = 0.0002\n",
    "    # beta denotes the regularization parameter used by gradient descent\n",
    "    beta = 0.02\n",
    "    # iterations denotes the number of iterations of gradient descent\n",
    "    iterations = 500\n",
    "\n",
    "    # perform 500 iterations of gradient descent\n",
    "    # P and Q can both be updated through iterations until the error converges to its local minimum (<0.001)\n",
    "    for iteration in range(iterations):\n",
    "        # iterate over the user-rating matrix\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                if R[i][j] > 0:\n",
    "                    # calculate the error\n",
    "                    eij = R[i][j] - numpy.dot(P[i,:],Q[:,j])\n",
    "\n",
    "                    for k in range(K):\n",
    "                        # calculate gradient with alpha and beta parameter\n",
    "                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
    "                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
    "\n",
    "        # compute the dot product of the matrices P and Q\n",
    "        eR = numpy.dot(P,Q)\n",
    "        \n",
    "        # initialize the error\n",
    "        e = 0\n",
    "    \n",
    "        # calculate the error\n",
    "        for i in range(len(R)):\n",
    "\n",
    "            for j in range(len(R[i])):\n",
    "\n",
    "                if R[i][j] > 0:\n",
    "\n",
    "                    e = e + pow(R[i][j] - numpy.dot(P[i,:],Q[:,j]), 2)\n",
    "\n",
    "                    for k in range(K):\n",
    "\n",
    "                        e = e + (beta/2) * (pow(P[i][k],2) + pow(Q[k][j],2))\n",
    "                        \n",
    "        # stop gradient descent if the error converges to its local minimum (< 0.001) \n",
    "        if e < 0.001:\n",
    "            break\n",
    "            \n",
    "    Q = Q.T\n",
    "    # compute the final user-rating matrix and return this matrix\n",
    "    nR = numpy.dot(P, Q.T)\n",
    "    return nR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebcda008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rating from User 1 of topic 1: 4.0\n",
      "Estimated rating from User 1 of topic 1: 3.4703481495884634\n",
      "Estimated rating from User 121 of topic 2: 4.6\n",
      "Estimated rating from User 121 of topic 2: 4.4296923807216775\n",
      "Snippet of result matrix:\n",
      "[[3.47034815 3.83017351 4.05417249 3.74525239 3.86464074]\n",
      " [3.0025354  3.29682384 3.67092534 3.01741231 3.59319616]\n",
      " [2.87460188 3.77041979 3.33502218 3.28419803 3.25597761]\n",
      " ...\n",
      " [2.80830211 3.02412361 3.35346654 2.74489136 3.20137839]\n",
      " [3.22831835 3.36947189 3.58654634 3.26339422 3.33855978]\n",
      " [3.36917483 3.0253206  3.33371442 3.21677487 3.82202904]]\n"
     ]
    }
   ],
   "source": [
    "np_array = numpy.array(matrix)\n",
    "# call the matrix factorization method\n",
    "rating_matrix = mft(np_array)\n",
    "# To check the performance of matrix factorization, print some original and estimated ratings:\n",
    "print(\"Original rating from User 1 of topic 1: \" + str(matrix[0][0]))\n",
    "print(\"Estimated rating from User 1 of topic 1: \"+ str(rating_matrix[0][0]))\n",
    "print(\"Estimated rating from User 121 of topic 2: \" + str(matrix[120][1]))\n",
    "print(\"Estimated rating from User 121 of topic 2: \"+ str(rating_matrix[120][1]))\n",
    "# print a snippet of the result matrix\n",
    "print(\"Snippet of result matrix:\")\n",
    "print(rating_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb1455e",
   "metadata": {},
   "source": [
    "<h2>Measure Cosine Similarity</h2>\n",
    "\n",
    "With the help of the library sklearn, the user similarity is computed by using the cosine similarity measure. After that, for each user the top-10 most similar users (users with the highest similarity value) are being collected and written on the recommendation list for the corresponding user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1efeff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the rating matrix into a pandas dataframe\n",
    "df = pd.DataFrame(rating_matrix)\n",
    "# measure the cosine similarity using sklearn\n",
    "cos_sim = cosine_similarity(df,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f50f6bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendation list for user PeterGleick:\n",
      "['DERVIEMOO', 'CindyBearsDen', 'Infamous_Raptor', 'ETObligations', 'Mango_News', 'weweia', 'Super_Said', '_covid_19_bot_', 'TheReviewWire', 'BradHopwood']\n",
      "Recommendation list for user pirone_art:\n",
      "['KODIMANBUILDS05', 'Fandoro4U', 'RevolutionsCen', 'JLawsFunhouse', 'MascotMotors', 'MGDezigns', 'MrsimpleJesse', 'zerostriker_', 'screenshotmag', 'antonio_m_reed']\n",
      "Recommendation list for user ChristopherNFox:\n",
      "['pm_girl', 'vthomas14', 'RCDavieHonda', 'Workshedanimate', 'PartsTown', 'scotgov', 'CovidFactoid', 'UpulTR', 'Tokyo_Tom', 'phtosbyakhughes']\n"
     ]
    }
   ],
   "source": [
    "numOfRows = cos_sim.shape[0]\n",
    "numOfRow = 0\n",
    "# the top 10 most similar user should be recommended for each user\n",
    "k = 10\n",
    "# the recommendation list stores the top k most similar users for each user\n",
    "recommendationlist = []\n",
    "\n",
    "# iterate over all rows of the cosine similarity result\n",
    "while numOfRow < numOfRows:\n",
    "    # the current row\n",
    "    row = cos_sim[numOfRow,:]\n",
    "    \n",
    "    # create a copy of the current row\n",
    "    # the copy is needed, because after finding the most similar user in this row, the most similar user is being deleted from\n",
    "    # the copy, to be able to find the next most similar user in the next iteration\n",
    "    # however the original row should stay the same, because the indices of users will be manipulated otherwise\n",
    "    tempRow = cos_sim[numOfRow,:]\n",
    "    \n",
    "    i = 0\n",
    "    # topkRow saves the top k most similar users only for the current user\n",
    "    topkRow = []\n",
    "    # iterate until the top k most similar users have been found\n",
    "    while i < k:\n",
    "        # find the most similar user of the current user in the copy of the row\n",
    "        max = numpy.amax(tempRow)\n",
    "        # get the index of the most similar user in the original row\n",
    "        index = numpy.where(row == max)[0]\n",
    "        \n",
    "        # check whether the most similar user is a different user than the current user\n",
    "        # the most similar user to the current user will always be the user himself (cosine similarity == 1)\n",
    "        # this case is being ignored\n",
    "        if index != numOfRow:\n",
    "            # the name of the most similar user is being appended to the topk list of the current user\n",
    "            name = users[index[0]]\n",
    "            topkRow.append(name)\n",
    "            # a recommendation has been found, therefore increase i by 1\n",
    "            i += 1\n",
    "        \n",
    "        # delete the most similar user from the copy of the row\n",
    "        tempRowIndex = numpy.where(tempRow == max)[0]\n",
    "        tempRow = numpy.delete(tempRow,tempRowIndex)\n",
    "    \n",
    "    # after the top k most similar users for a user have been found, append those to the recommendation list, which stores the \n",
    "    # top k most similar users for each user\n",
    "    recommendationlist.append(topkRow)\n",
    "    numOfRow += 1\n",
    "\n",
    "# print recommendation lists of the first three users (printing the recommendations lists for all users would be a big print)\n",
    "print(\"Recommendation list for user \" + users[0] + \":\")\n",
    "print(recommendationlist[0])\n",
    "print(\"Recommendation list for user \" + users[1] + \":\")\n",
    "print(recommendationlist[1])\n",
    "print(\"Recommendation list for user \" + users[2] + \":\")\n",
    "print(recommendationlist[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532d82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "055a310f58a1ab811f7fd85f388fd7463c92f0a3a43a1e2841d1b26142fb7dc4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
